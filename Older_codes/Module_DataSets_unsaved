import matplotlib.pyplot as plt
import numpy as np
import os
from pathlib import Path
from tqdm import tqdm
import pandas as pd
from astropy.io import fits
from astropy.table import Table
from astropy import units as u
from astropy.coordinates import SkyCoord
# from astropy.visualization import astropy_mpl_style

------------------------------------

catalog (str): name of the source of light curves
event (dict of bands with 3 arrays per each?): refer to the join of light curves of each band of the same point of the sky at the same epoch. Each event is assigned a snid
band (str): band
snid (str): id number for the light curve (assigned by ELAsTiCC)

cuts (list of str): List which cointains the names of the cut (referenced to a function)

# Light curves management

## Extraction and plotting

------------------------------------

%%time
from pathlib import Path
from tqdm import tqdm
import os
import pandas as pd
import numpy as np
from astropy.io import fits
import matplotlib.pyplot as plt

def flux_to_mag(Flux, Flux_err = ""):
    '''Use numpy arrays, no list'''
    if type(Flux_err) != str:
        print("entro")
        top = Flux + Flux_err
        bot = Flux - Flux_err
        top_mag = 27.5-2.5*np.log10(np.abs(top))
        bot_mag = 27.5-2.5*np.log10(np.abs(bot))
        return 27.5-2.5*np.log10(np.abs(Flux)), np.abs(top_mag-bot_mag)
    else:
        return 27.5-2.5*np.log10(np.abs(Flux))

def plot_lc(mjd, mag, mag_err, bands, title = None):
    bands_colors = {'u':'b', 'g':'c', 'r':'g', 'i':'orange', 'z':'r', 'y':'m'}
    plt.errorbar(mjd, mag, mag_err, color=bands_colors[band.lower], marker='o', mew = 0.05,linestyle='--', lw = 1, label = band[0])
    plt.gca().invert_yaxis()
    plt.xlabel("Epoch (MJD)")
    plt.ylabel("Magnitude")
    if title != None:
        plt.title(title)
    
def event_fits(catalog, class_name, fit_file, i = None, snid = None, bands = "ugrizY", plot = False, median = False):
    fits_files =[file for file in os.listdir(Path(catalog, class_name)) if file.endswith("FITS.gz")]
    pref = os.path.commonprefix(fits_files)
    head = fits.open(Path(catalog, class_name, pref+f"{fit_file}_HEAD.FITS.gz"))[1].data
    phot = fits.open(Path(catalog, class_name, pref+f"{fit_file}_PHOT.FITS.gz"))[1].data
    if i != None:
        start = head["PTROBS_MIN"][i]
        end = head["PTROBS_MAX"][i]
        snid = head["SNID"][0]
    elif snid != None:
        snid = str(snid)
        start = head[head["SNID"]==snid]["PTROBS_MIN"][0]
        end = head[head["SNID"]==snid]["PTROBS_MAX"][0]
        i = snid
    lc = phot[start:end]
    for band in bands:
        lc_band = lc[lc["BAND"] == band]
        mjd = lc_band["MJD"]
        mag, mag_err = flux_to_mag(lc_band["FLUXCAL"],lc_band["FLUXCALERR"])
        if plot == True:
            bands_colors = {'u':'b', 'g':'c', 'r':'g', 'i':'orange', 'z':'r', 'y':'m'}
            plt.errorbar(mjd, mag, mag_err, color=bands_colors[band.lower], marker='o', mew = 0.05,linestyle='--', lw = 1, label = band[0])
            plt.gca().invert_yaxis()
    if plot == True:
        plt.xlabel("Epoch (MJD)")
        plt.ylabel("Magnitude")
        plt.title(class_name+" "+fit_file+" "+str(i))
        if median == True:
            plt.hlines(np.median(mag), xmin=min(mjd), xmax=max(mjd), label = "median")
        if type(plot)==list:
            for (m, name) in plot:
                plt.hlines(m, xmin=min(mjd), xmax=max(mjd), label = name)
        plt.legend()
        plt.show()
    return mjd, mag, mag_err, snid

def event_dat(catalog, class_name, snid, bands = "ugrizY", form=".dat", plot = False, median = False):
    for band in bands:
        dataset = catalog+"_"+band
        mjd, mag, mag_err = np.loadtxt(Path(dataset, class_name, f"lc_{snid}_{band}{form}")).T
        if plot != False:
            bands_colors = {'u':'b', 'g':'c', 'r':'g', 'i':'orange', 'z':'r', 'y':'m'}
            plt.errorbar(mjd, mag, mag_err, color=bands_colors[band.lower()], marker='o', mew = 0.05,linestyle='--', lw = 1, label = band[0])
            plt.gca().invert_yaxis()
    if plot != False:
        plt.xlabel("Epoch (MJD)")
        plt.ylabel("Magnitude")
        plt.title(class_name+" "+str(snid))
        if median == True:
            plt.hlines(np.median(mag), xmin=min(mjd), xmax=max(mjd), label = "median")
        if type(plot)==list:
            for (m, name) in plot:
                plt.hlines(np.median(m), xmin=min(mjd), xmax=max(mjd), label = name)
        plt.legend()
        plt.show()
    return mjd, mag, mag_err, snid

------------------------------------

mjd, mag, mag_err, snid = event_dat("DataSets/ELASTICC2", "Cepheid", 10006766, bands = "i", plot=True)
sorted(mjd)

------------------------------------

# Filters

------------------------------------

import numpy as np
import os

bandcolor = {'u':'b', 'g':'c', 'r':'g', 'i':'orange', 'z':'r', 'y':'m'}
m_sat = {'u':14.7, 'g': 15.7, 'r': 15.8, 'i': 15.8, 'z': 15.3, 'y': 13.9}
M5 = {'u':23.78, 'g':24.81, 'r':24.35 , 'i':23.92, 'z': 23.34 , 'y':22.45}

def filter_m_sat(mjd, m, merr, m_sat):
    try:
        new_mjd, new_m, new_merr= np.array([(mjd_, m_, merr_) for mjd_, m_, merr_ in zip(mjd, m, merr) if m_ >= m_sat]).T
    except ValueError:
        new_mjd, new_m, new_merr = np.array([]), np.array([]), np.array([])
    del_points = len(m)-len(new_m)
    return new_mjd, new_m, new_merr, del_points

def filter_m_5(mjd, m, merr, m_5, N_points = 10, N_sigma = 1):
    n_high_snr = sum([1 for mjd_, m_, merr_ in zip(mjd, m, merr) if m_+N_sigma*merr_<m_5])
    stat = n_high_snr >= N_points
    return stat
    
def filter_has_peak(mjd, m, merr, N_points = 4, N_sigma = 3):
    median = np.median(m)
    n_high_m = sum([1 for mjd_, m_, merr_ in zip(mjd, m, merr) if m_+N_sigma*merr_<median])
    return n_high_m >= N_points

def filter_chi2_dof(m, merr, cte, upper_limit = None):
    chi = ((m-cte)**2)/(merr**2)
    if upper_limit == None:
        return np.sum(chi)/len(m)
    else:
        return np.sum(chi)/len(m) > upper_limit

def filter_has_variability(mjd, m, merr, N_points = 4, N_sigma = 3):
    median = np.median(m)
    n_distant_m = sum([1 for mjd_, m_, merr_ in zip(mjd, m, merr) if abs(m_-median)>N_sigma*merr_])
    return n_distant_m > N_points

def apply_filters(lc_path, m_5 = None, m_sat = None,  upper_limit=2,filters = "all"):
    header = "lc_path\t"
    result = [lc_path]
    mjd, m, merr = np.loadtxt(lc_path).T
    if filters == "all":
        filters = ["m_sat", "m_5", "has_peak", "high_chi2", "has_variability"]
    if "m_sat" in filters:
        mjd, m, merr, del_points = filter_m_sat(mjd, m, merr, m_sat)
        header+="del_points\t"
        result+=[del_points]
    if "m_5" in filters:
        valid = filter_m_5(mjd, m, merr, m_5, N_points = 10, N_sigma = 1)
        header+="{}\t".format("valid") 
        result+=[valid]
    if "high_chi2" in filters:
        high_chi2 = filter_chi2_dof(m, merr, np.median(m), upper_limit = upper_limit)
        header+="{}\t".format("high_chi2") 
        result+=[high_chi2]
    if "has_peak" in filters:
        has_peak = filter_has_peak(mjd, m, merr, N_points = 4, N_sigma = 3)
        header+="{}\t".format("has_peak") 
        result+=[has_peak]
    if "has_variability" in filters:
        has_variability = filter_has_variability(mjd, m, merr, N_points = 4, N_sigma = 3)
        header+="{}\t".format("has_variability")
        result+=[has_variability]
    return mjd, m, merr, tuple(result), header[:-1]

------------------------------------

import os
import pandas as pd
from pathlib import Path
import matplotlib.pyplot as plt

fil_results = [i for i in os.listdir("DataSets/ELASTICC2_i") if i.startswith("filters")]

fig, ax = plt.subplots(figsize=(12, 6))

median_data = [] 
labels = []  

for fil in fil_results:
    df = pd.read_csv(Path("DataSets", "ELASTICC2_i", fil), sep="\t")
    label = fil.split(".")[0].split("_")[-1]
    if label == "i":
        label = "Constant"
    median = df['median'].dropna() 
    median_data.append(median)
    labels.append(label)
plot = [(m_5["i"], "$m_5$", "green"), (m_sat["i"], "$m_{sat}$", "red")]

ax_2 = ax.twiny()

counts = [len(median) for median in median_data]
bar_positions = [i + 1 for i in range(len(labels))]

ax.barh(bar_positions, counts, color='lightblue', alpha = 0.6)
ax.set_xlabel('Count')

ax_2.boxplot(median_data, labels=labels, vert=False)
ax_2.set_xlabel('Median Value')
ax_2.set_title('Boxplot of the median values distribution for Each Category (i band)')
for (m, name, color) in plot:
    ax_2.axvline(m, color = color, label = name)
ax_2.legend(loc = (0.9,0.5))

plt.show()

------------------------------------


## DataSet
# Properties

Amount of light curves per category of a data set

------------------------------------

# Calculate the class (folder) with min amount of light curves
from astropy.io import fits
from tqdm import tqdm
import os
from pathlib import Path
import matplotlib.pyplot as plt

def count_lc(catalog, data_format = "dat"):
    n_lc={}
    class_names = [i for i in  os.listdir(catalog) if not "." in i]
    if data_format == "fits":
        for folder in tqdm(class_names, desc= f"Counting light curves per category of {catalog}"):
            filenames_HEAD = [file for file in os.listdir(Path(catalog, folder)) if '_HEAD.FITS' in file]
            n = []
            for head in filenames_HEAD:
                with fits.open(Path(catalog,folder,head)) as hdu:
                    n.append(len(hdu[1].data))
            n_lc[folder]=sum(n)
    if data_format == "dat":
        for folder in tqdm(class_names, desc=f"Counting light curves per category of {catalog}"):
            if folder == "proc":
                continue
            lc_class = [lc for lc in os.listdir(Path(catalog, folder)) if 'lc_' in lc]
            n_lc[folder]=len(lc_class)
    return n_lc

def plot_count_lc(n_lc, kind="bar", title = None,  gal = False ):
    labels, values = list(n_lc.keys()), list(n_lc.values())
    labels = [i[len(os.path.commonprefix(labels)):] for i in labels]
    if kind == "bar":
        n = len(labels)
        plt.figure(figsize = (10,n*0.2))
        if gal == True:
            gal = ["Cepheid", "d-Sct", "dwarf-nova","EB", "Mdwarf-flare", "RRL", "uLens-Binary", 
                   "uLens-Single-GenLens", "uLens-Single_PyLIMA"]
            extra_gal = [lab for lab in labels if lab not in gal]
            val_gal = []; val_extra_gal = []
            for val, lab in zip(values, labels):
                if lab in gal:
                    val_gal.append(val)
                else:
                    val_extra_gal.append(val)
            if (len(val_gal) == 0) or (len(val_extra_gal) == 0):
                plt.barh(labels,values, height=0.4)
            else:
                plt.barh(gal, val_gal, height=0.4, label = "Galactic")
                plt.barh(extra_gal, val_extra_gal, height=0.4, label = "Extragalactic")
                plt.legend()

        else:        
            plt.barh(labels,values, height=0.4)
        plt.grid(axis="x")
        plt.xlabel("Light curves")
        for i, (label, value) in enumerate(zip(labels, values)):
            if value == max(values):
                plt.text(value*(4.5/5), labels[i-1], str(value), va='center')
            else:
                plt.text(value, label, str(value), va='center')
        plt.ylim(-0.5, len(labels))
        plt.xlim(0, max(values)*1.01)
    if kind == "pie":
        wedges, label, j = plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=90)
        for i, (wedge, label) in enumerate(zip(wedges, labels)):
            theta = (wedge.theta2 - wedge.theta1) / 2.0 + wedge.theta1
            x = 0.9 * np.cos(theta * np.pi / 180)
            y = 0.9 * np.sin(theta * np.pi / 180)
            plt.text(x*0.95, y*0.95, f'{values[i]}', ha='center', va='center')
            plt.plot([x*1.05, 1.15 * x], [y*1.05, 1.15 * y], color='black', linestyle='--', linewidth=0.5)
    #         plt.legend(labels, loc=(1,1)) 
    plt.title(title)
    plt.show()

# n_lc = get_n_lc("ELASTICC2_fits", data_format = "fits")

n_lc = count_lc("DataSets/ELASTICC2_i_Valid_Chi_Constant", data_format = "dat")
sizes = plot_count_lc(n_lc, kind = "bar", title = "ELASTICC2_i_Valid_Chi_Constant", gal = False)

------------------------------------

Size of each category of a dataset.

------------------------------------

import os
import matplotlib.pyplot as plt
from pathlib import Path
from tqdm import tqdm

def get_sizes(catalog, data_format = "dat"):
    categories_sizes = {}
    class_names = [folder for folder in os.listdir(catalog) if not "." in folder]
    for folder in tqdm(class_names, desc = "Computing size of categories"):
        folder_path = Path(catalog, folder)
        categories_sizes[folder] = sum(f.stat().st_size for f in folder_path.glob('**/*') if f.is_file()) / (1024 * 1024)
    return categories_sizes

def plot_sizes(categories_sizes, title="Folder Sizes"):
    labels, sizes = list(categories_sizes.keys()), list(categories_sizes.values())
    labels = [i[len(os.path.commonprefix(labels)):] for i in labels]
    n = len(labels)
    plt.figure(figsize=(10, n * 0.25))

    plt.barh(labels, sizes, height=0.4)
    plt.grid(axis="y")
    plt.xlabel("Folder Size (MB)")
    plt.xlim(0, max(sizes)*1.01)
    plt.ylim(-0.5, len(labels))

    for i, (label, size) in enumerate(zip(labels, sizes)):
        if size == max(sizes):
            plt.text(size*(4.5/5), labels[i-1], f'{size:.2f} MB', va='center')
        else:
            plt.text(size, label, f'{size:.2f} MB', va='center')

    plt.title(title + f" Total: {sum(sizes):.2f} MB")
    plt.show()
dataset_name = "ELASTICC2_i_Valid_Chi_Constant"
folder_sizes = get_sizes("DataSets/"+dataset_name)
plot_sizes(folder_sizes, title=dataset_name)

------------------------------------

import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import seaborn as sns
from tqdm import tqdm
from pathlib import Path

def count_lc(catalog, data_format = "dat"):
    n_lc={}
    class_names = [i for i in  os.listdir(catalog) if not "." in i]
    if data_format == "fits":
        for folder in tqdm(class_names, desc= f"Counting light curves per category of {catalog}"):
            filenames_HEAD = [file for file in os.listdir(Path(catalog, folder)) if '_HEAD.FITS' in file]
            n = []
            for head in filenames_HEAD:
                with fits.open(Path(catalog,folder,head)) as hdu:
                    n.append(len(hdu[1].data))
            n_lc[folder]=sum(n)
    if data_format == "dat":
        for folder in tqdm(class_names, desc=f"Counting light curves per category of {catalog}"):
            lc_class = [lc for lc in os.listdir(Path(catalog, folder)) if 'lc_' in lc]
            n_lc[folder]=len(lc_class)
    return n_lc

def count_subdatasets_lc(catalog):
    data = {}
    for dataset in [i for i in os.listdir("DataSets") if (catalog in i) and (not "." in i)]:
        data[dataset] = count_lc("DataSets/"+dataset, data_format = "dat")
    return data

def plot_balance_subdatasets(data, title="", kind="bar", cuts=False):
    '''kind: "bar" or "heatmap"
       cuts: only for "bar".'''
    datasets = list(data.keys())
    datasets_labels = [i[len(os.path.commonprefix(list(data.keys())))+1:] for i in list(data.keys())]
    if kind == "bar":
        bar_positions = range(len(datasets))
        bottom_values = [0] * len(datasets)  # Inicializa la variable "bottom_values" en ceros
        categories = []
        for dataset in datasets:
            cat = [i for i in data[dataset].keys() if i!="proc"]
            if len(cat) > len(categories):
                categories = cat

        if cuts == True:
            groups_filters = {"Valid":  ["valid"], "Chi":["high_chi2"], "Peak":["has_peak"],"Variab": ["has_variability"], "Constant": ["Constant"], "Custom": ["Custom"]}

            filter_matrix = np.zeros((len(groups_filters), len(datasets)), dtype=bool)
            for i, dataset in enumerate(datasets):
                for j, cut in enumerate(groups_filters):
                    if cut in dataset:
                        filter_matrix[j, i] = True

            fig, (ax1, ax2) = plt.subplots(2, 1, gridspec_kw={'height_ratios': [3, 1]})
            plt.subplots_adjust(hspace=0.1)
            ax1.set_title(title)
            bar_width = 0.5  
            bar_positions = np.arange(len(datasets))  
            bottom = np.zeros(len(datasets))    
            for i, category in enumerate(categories):
                category_data = []
                for dataset in datasets:
                    try:
                        category_data.append(data[dataset][category])
                    except:
                        category_data.append(0)
                ax1.bar(bar_positions, category_data, label=category, bottom=bottom, width=bar_width)
                bottom += np.array(category_data)  

            ax1.set_xticks(bar_positions)
            ax1.set_xlim(-0.5,len(bottom))
            ax1.set_xticklabels([])  
            ax2.set_xticks(bar_positions)  
            ax2.set_xticklabels([]) #datasets_labels, rotation=90)
            ax2.set_xlim(-0.5,len(bottom))
            ax2.grid()
            for i in range(len(groups_filters)):
                for j in range(len(datasets)):
                    if filter_matrix[i, j]:
                        ax2.scatter(j, i, marker='o', s=50, color='blue', edgecolor='black', linewidth=1)  # Agregar borde negro a los c√≠rculos

            ax2.set_yticks(range(len(groups_filters)))
            ax2.set_ylabel("Cuts")
            ax2.set_yticklabels(groups_filters, rotation=0)
            ax2.set_xlabel('DataSets')
            ax2.set_ylim(-0.5,len(groups_filters)-0.5)
            ax1.set_ylabel('Light curves per category')
            ax1.legend(loc=(1.01, 0.2))
        else:
            for i, category in enumerate(categories):
                try:
                    category_data = [data[dataset][category] for dataset in datasets]
                except:
                    pass
                plt.bar(bar_positions, category_data, label=category, bottom=bottom_values)
                bottom_values = [bottom_values[j] + category_data[j] for j in range(len(datasets))]
            plt.title(title)
            plt.xlabel('Datasets')
            plt.ylabel('Light curves per category')
            plt.xticks(bar_positions, datasets_labels, rotation = 90)
            plt.legend(loc = (1.05,0.2))
    if kind == "heatmap":
        replace_dict = {key: new_label for key, new_label in zip(data.keys(), datasets_labels)}
        data_new = {replace_dict[key]: value for key, value in data.items()}
        df = pd.DataFrame(data_new)
        df.fillna(0, inplace=True) 
        df = df.transpose()
        sns.set(style="whitegrid")
        plt.figure(figsize=(12, 8))
        ax = sns.heatmap(df, annot=True, fmt="g", cmap="BuPu", cbar=False, linewidths=0.5)
        plt.xlabel("Categories")
        plt.ylabel("Datasets")
        plt.title(title+ " - Amount of light curves")

# data = count_subdatasets_lc("ELASTICC2_i")
plot_balance_subdatasets(data, kind="heatmap", title="ELASTICC2_i", cuts = True) # kind = "bar"

------------------------------------

import os
import random
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm

root_dir = "DataSets"

output_dir = "LightCurves_Visualization"

bandcolor = {'u':'b', 'g':'c', 'r':'g', 'i':'orange', 'z':'r', 'y':'m'}
m_sat = {'u':14.7, 'g': 15.7, 'r': 15.8, 'i': 15.8, 'z': 15.3, 'y': 13.9}
m_5 = {'u':23.78, 'g':24.81, 'r':24.35 , 'i':23.92, 'z': 23.34 , 'y':22.45}

def select_and_visualize_random_curves(dataset_dir, category_dir, n=15):
    data_dir = os.path.join(root_dir, dataset_dir, category_dir)
    files = os.listdir(data_dir)
    txt_files = [file for file in files if file.endswith(".dat") or file.endswith(".txt") ]

    if len(txt_files) < n:
        print(f"Not enough light curves to plot in {dataset_dir}/{category_dir}.")
        return

    selected_files = random.sample(txt_files, n)

    plt.figure(figsize=(24, 12))
    plt.suptitle(f"Path: {os.path.join(root_dir, dataset_dir, category_dir)}", fontsize=14)

    for i, file in enumerate(selected_files):
        file_path = os.path.join(data_dir, file)
        data = np.loadtxt(file_path).T
        mjd, mag, mag_err = data
        plt.subplot(3, 5, i+1)  # 3 filas, 5 columnas para los 15 subplots
        plt.gca().invert_yaxis()
        plt.errorbar(mjd, mag, yerr=mag_err, color="grey", lw = 0.5, marker="o", mfc="darkblue", mec="darkblue", markersize=3, elinewidth=1, ecolor="darkblue")
        snid = [i for i in file_path.split(".")[0].split("_") if i.isdigit()]
        plt.title(f"Light curve ID: {snid[0]}")
        plt.xlabel("Epoch (MJD)")
        plt.ylabel("Magnitude")
        plt.grid()
        plt.hlines(np.median(mag), xmin=min(mjd), xmax=max(mjd), color = "red", label = "median")
        plot = [(m_5["i"], "$m_5$", "green"), (m_sat["i"], "$m_{sat}$", "darkorange")]
        for (m, name, color) in plot:
            plt.hlines(m, xmin=min(mjd), xmax=max(mjd), color = color, label = name)
        plt.legend()

    plt.tight_layout()
    category_name = category_dir.replace(" ", "_")
    output_dataset_dir = os.path.join(output_dir, dataset_dir)
    os.makedirs(output_dataset_dir, exist_ok = True)
    output_file = os.path.join(output_dataset_dir, f"{category_name}_curves_visualization.png")
    plt.savefig(output_file)
    plt.close()

plt.ioff()  # Deactivate interactive figure visualization

# Iteration through datasets and categories:
for dataset_dir in os.listdir(root_dir):
    if "ELASTICC2_i" in dataset_dir and os.path.isdir(os.path.join(root_dir, dataset_dir)):
        print(dataset_dir)
    for category_dir in tqdm(os.listdir(os.path.join(root_dir, dataset_dir)), desc=f"{dataset_dir} categories"):
        if not "checkpoints" in category_dir and os.path.isdir(os.path.join(root_dir, dataset_dir, category_dir)):
            select_and_visualize_random_curves(dataset_dir, category_dir)
            
            
------------------------------------

## Format transformation 
ELAsTiCC (fits) to MicroLIA(dat)

------------------------------------

from pathlib import Path
from astropy.io import fits

def write_lc_file(Mjd, Mag, Mag_err, lc_path):
    with open(lc_path, 'w') as f:
        for mjd, mag, mag_err in zip(Mjd, Mag, Mag_err) :
            f.write(str('   '+"%.3f" %mjd)+'  '+str("%.4f" %mag)+'   '+str("%.3f" %mag_err)+'\n')
        f.write('\n')

dataset_path = "ELASTICC_2"
name = "ELASTICC2"
bands = "i" # "iruzgY"

# categ_folders = [folder for folder in os.listdir(dataset_path) if "ELASTICC2_TRAIN" in folder]
# categ_folders = ['ELASTICC2_TRAIN_02_RRL', 'ELASTICC2_TRAIN_02_Mdwarf-flare',  'ELASTICC2_TRAIN_02_uLens-Binary',
#      'ELASTICC2_TRAIN_02_uLens-Single-GenLens','ELASTICC2_TRAIN_02_uLens-Single_PyLIMA', 
#      'ELASTICC2_TRAIN_02_Cepheid', 'ELASTICC2_TRAIN_02_d-Sct', 'ELASTICC2_TRAIN_02_EB', 'ELASTICC2_TRAIN_02_dwarf-nova']

def fits_to_dat(dataset_path, name, bands, categ_folders):
    n_categs = len(categ_folders)
    heading= len(os.path.commonprefix(categ_folders))
    for band in bands:                                        # Data set of each band
        run_name = str(name+"_"+band)
        Path(run_name).mkdir(exist_ok=True)
        print("Filter band: ", band)
        for j, categ_name in enumerate(categ_folders):    # Folders of each class
            Path(run_name, class_name[heading:]).mkdir(exist_ok=True)
            print("--------------------------------------------------------------------")
            print(f"Folder: {categ_name}    {j+1}/{n_categs}")
            if j==0:
                fits_files =[file for file in os.listdir(Path(dataset_path, categ_name)) if file.endswith("FITS.gz")]
                pref = os.path.commonprefix(fits_files)
            for i_fit in tqdm(range(40)):                           # .fit files of each simulation
                fit = '{:02d}'.format(i_fit+1)
                i_lc = 0
                head = fits.open(Path(dataset_path, categ_name, pref+f"{fit}_HEAD.FITS.gz"))[1].data
                for snid in head.SNID:
                    Mjd, Mag, Mag_err, snid= event_fits(dataset_path, categ_name, fit, snid=snid, bands=band, plot=False)
                    lc_path = Path(run_name, categ_name[heading:], "lc_"+snid+"_"+ band +'.dat')
                    write_lc_file(Mjd, Mag, Mag_err, lc_path)

------------------------------------
                
# Simulation of constant light curves


------------------------------------

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import os
from copy import copy
from tqdm import tqdm
import rubin_sim
import rubin_sim.maf as maf # LSST's Metrics Analysis Framework (MAF).
from rubin_sim.phot_utils import signaltonoise
from rubin_sim.phot_utils import PhotometricParameters
from rubin_sim.phot_utils import bandpass_dict
from rubin_sim.data import get_baseline
import astropy.units as u
from astropy.coordinates import SkyCoord

def LSST_cadence_noise(ra, dec, opsim = "baseline"):
    if opsim == "baseline":
        opsim = get_baseline()
    photParams = PhotometricParameters(exptime=30,nexp=1, readnoise=None) # Photometry: Exposure seconds and times
    LSST_BandPass = bandpass_dict.BandpassDict.load_total_bandpasses_from_files() # Info about filters
    # default5sigma = {'u':23.78, 'g':24.81, 'r':24.35 , 'i':23.92, 'z': 23.34 , 'y':22.45} # 5-sigma depth is a measure of the faintest object (in terms of magnitude) that can be detected in an image with a 5-sigma confidence. 
    metric = rubin_sim.maf.metrics.PassMetric(cols=['filter', 'observationStartMJD', 'fiveSigmaDepth']) # Metrics interested in
    slicer = maf.slicers.UserPointsSlicer(ra=[ra], dec=[dec]) # # Locations interested in - Slice the sky in the point ra,dec
    sql = ''
    bundleMetrics = maf.MetricBundle(metric, slicer, sql) # Metrics ran per slice_point (dict)
    bundle = maf.MetricBundleGroup([bundleMetrics], opsim, out_dir="temp") #This generate ResultsDb with the metrics and stats
    bundle.run_all(plot_now=False) # write metrics in MetricBundle.metric_values #Try plot now to show metrics results
    dataSlice = bundleMetrics.metric_values[0]
    return dataSlice, LSST_BandPass, photParams

def constant_lc_simulation(M_input, dataSlice, LSST_BandPass, photParams, mjd_min = 60410.388, mjd_max = 61317.016, save_path = None):
    '''M_input: {band: m_band}'''
    m5 = {}; MJD = {}; M = {}; Merr = {}
    for band in M_input.keys():
        MJD[band] = dataSlice['observationStartMJD'][np.where(dataSlice['filter'] == band.lower()) and np.where(dataSlice['observationStartMJD']<=mjd_max) and np.where(dataSlice['observationStartMJD']>=mjd_min)] 
        m5[band] = dataSlice['fiveSigmaDepth'][np.where(dataSlice['filter'] == band.lower())]
        Merr[band] = [signaltonoise.calc_mag_error_m5(M_input[band],  LSST_BandPass[band.lower()], M5 , photParams)[0] for M5 in m5[band]]
        M[band] = [np.random.normal(M_input[band],magerr) for magerr in Merr[band]]  
        # mags = dataSlice['fiveSigmaDepth'][np.where(dataSlice['filter'] == band)]
        if save_path != None:
            np.savetxt(save_path+'/lc_'+band.lower()+'.dat', np.array([MJD[band], M[band], Merr[band]]).T)
    return MJD, M, Merr

def stars_constant_lc_simulation(dataSlice, LSST_BandPass, photParams, stars_catalog, n_stars="all", n_0=0, out="save"):
    ''' 
    stars_file: Output of trilegal
    n_lc: Amount of light curves to simulate from de stars_file
    out: 
        -"save": Only save light curves in folders
        -"load": Only load light curves in the variable simulated_curves
        -"all": Save and load.
    '''
    catalog_name = stars_catalog.split("/")[-1].split(".")[0]
    M_input = pd.read_csv(stars_catalog, usecols=["u", "g", "r", "i", "z", "Y"], sep="\s+", decimal ='.')
    M_input = M_input.drop([len(M_input['u'])-1])
    print("Catalog: "+ catalog_name)
    for band in M_input.keys():
        name = f"{catalog_name}_{band}"
        if name not in os.listdir():
            os.mkdir(name)
    sim_curves = []          # List of tuples (mjd, mag, magerr, m5) wich are dictonaries
    for i_star in tqdm(range(n_0, n_stars)):
        mjd, mag, magerr = constant_lc_simulation(M_input.iloc[i_star], dataSlice, LSST_BandPass, photParams)
        if (out=="save") or (out=="all"):
            for band in M_input.keys():
                MJD = mjd[band]
                M = mag[band]
                Merr = magerr[band]
                np.savetxt(f'{catalog_name}_{band}/lc_{catalog_name}_{band}_{i_star}.dat', np.array([MJD, M, Merr]).T)
            del mjd, mag, magerr
        if (out == "load") or (out == "all"):
            sim_curves.append(dict(zip(["MJD","M", "Merr"], [mjd, mag, magerr])))
    if out != "save":    
        return M_input, sim_curves
    
# # OpSim-  Observation strategy: mjd, 5sigma
# opsim = get_baseline()

# # Location of the star
# galactic_center = SkyCoord(l=0*u.degree, b=0*u.degree, frame='galactic')
# coord = galactic_center.transform_to('icrs')
# ra, dec = coord.ra.deg, coord.dec.deg



# # # Simulation of 6000 constant lc
# dataSlice, LSST_BandPass, photParams = LSST_cadence_noise(ra, dec, opsim)
# stars_constant_lc_simulation(dataSlice, LSST_BandPass, photParams, stars_catalog = "DataSets/trilegal1.dat", n_0 = 10000, n_stars=15000, out="save")

------------------------------------

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import os
from copy import copy
from tqdm import tqdm
# import rubin_sim
# import rubin_sim.maf as maf # LSST's Metrics Analysis Framework (MAF).
# from rubin_sim.phot_utils import signaltonoise
# from rubin_sim.phot_utils import PhotometricParameters
# from rubin_sim.phot_utils import bandpass_dict
# from rubin_sim.data import get_baseline
# import astropy.units as u
# from astropy.coordinates import SkyCoord
# from MicroLIA.simulate import cv
M_sat = {'u':14.7, 'g': 15.7, 'r': 15.8, 'i': 15.8, 'z': 15.3, 'y': 13.9}
M_5 = {'u':23.78, 'g':24.81, 'r':24.35 , 'i':23.92, 'z': 23.34 , 'y':22.45}

def LSST_cadence_noise(ra, dec, opsim = "baseline"):
    if opsim == "baseline":
        opsim = get_baseline()
    photParams = PhotometricParameters(exptime=30,nexp=1, readnoise=None) # Photometry: Exposure seconds and times
    LSST_BandPass = bandpass_dict.BandpassDict.load_total_bandpasses_from_files() # Info about filters
    # default5sigma = {'u':23.78, 'g':24.81, 'r':24.35 , 'i':23.92, 'z': 23.34 , 'y':22.45} # 5-sigma depth is a measure of the faintest object (in terms of magnitude) that can be detected in an image with a 5-sigma confidence. 
    metric = rubin_sim.maf.metrics.PassMetric(cols=['filter', 'observationStartMJD', 'fiveSigmaDepth']) # Metrics interested in
    slicer = maf.slicers.UserPointsSlicer(ra=[ra], dec=[dec]) # # Locations interested in - Slice the sky in the point ra,dec
    sql = ''
    bundleMetrics = maf.MetricBundle(metric, slicer, sql) # Metrics ran per slice_point (dict)
    bundle = maf.MetricBundleGroup([bundleMetrics], opsim, out_dir="temp") #This generate ResultsDb with the metrics and stats
    bundle.run_all(plot_now=False) # write metrics in MetricBundle.metric_values #Try plot now to show metrics results
    dataSlice = bundleMetrics.metric_values[0]
    return dataSlice, LSST_BandPass, photParams

def cv_lc_simulation(M_input, dataSlice, LSST_BandPass, photParams, save_path = None):
    '''M_input: {band: m_band}'''
    m5 = {}; MJD = {}; M = {}; Merr = {}
    for band in M_input.keys():
        m5[band] = dataSlice['fiveSigmaDepth'][np.where(dataSlice['filter'] == band.lower())]
        MJD[band] = dataSlice['observationStartMJD'][np.where(dataSlice['filter'] == band.lower())] 
        Merr[band] = [signaltonoise.calc_mag_error_m5(M_input[band],  LSST_BandPass[band.lower()], M5 , photParams)[0] for M5 in m5[band]] 
        if save_path != None:
            np.savetxt(save_path+'/lc_'+band.lower()+'.dat', np.array([MJD[band], M[band], Merr[band]]).T)
    return MJD, M, Merr, baseline
band= "i"
# # Location of the star
# galactic_center = SkyCoord(l=0*u.degree, b=0*u.degree, frame='galactic')
# coord = galactic_center.transform_to('icrs')
# ra, dec = coord.ra.deg, coord.dec.deg
# # Get MJD abd M5
# dataSlice, LSST_BandPass, photParams = LSST_cadence_noise(ra, dec)
# MJD = dataSlice['observationStartMJD'][np.where(dataSlice['filter'] == band.lower())]
# M5 = dataSlice['fiveSigmaDepth'][np.where(dataSlice['filter'] == band.lower())]
# np.savetxt('lc_'+band.lower()+'.dat', np.array([MJD, M5]).T)

# MJD, M, Merr, baseline = cv_lc_simulation(M_input, dataSlice, LSST_BandPass, photParams)

# plot_lc(mjd, mag, mag_err, bands, title = f"Cataclysmic Variable - Baseline: {baseline}")

------------------------------------

M_input = {}
MJD, M5 = np.loadtxt("lc_i.dat").T 
from MicroLIA.simulate import cv
baseline = np.random.uniform(M_sat[band], np.mean(M5), len(MJD)) 
M_input[band] = cv(MJD, baseline)[0]
np.savetxt('lc_'+band.lower()+'1.dat', np.array([MJD, M_input[band]]).T)

------------------------------------

import rubin_sim
import rubin_sim.maf as maf # LSST's Metrics Analysis Framework (MAF).
from rubin_sim.phot_utils import signaltonoise
from rubin_sim.phot_utils import PhotometricParameters
from rubin_sim.phot_utils import bandpass_dict
from rubin_sim.data import get_baseline
import astropy.units as u
from astropy.coordinates import SkyCoord
# from MicroLIA.simulate import cv
M_sat = {'u':14.7, 'g': 15.7, 'r': 15.8, 'i': 15.8, 'z': 15.3, 'y': 13.9}
M_5 = {'u':23.78, 'g':24.81, 'r':24.35 , 'i':23.92, 'z': 23.34 , 'y':22.45}

def LSST_cadence_noise(ra, dec, opsim = "baseline"):
    if opsim == "baseline":
        opsim = get_baseline()
    photParams = PhotometricParameters(exptime=30,nexp=1, readnoise=None) # Photometry: Exposure seconds and times
    LSST_BandPass = bandpass_dict.BandpassDict.load_total_bandpasses_from_files() # Info about filters
    # default5sigma = {'u':23.78, 'g':24.81, 'r':24.35 , 'i':23.92, 'z': 23.34 , 'y':22.45} # 5-sigma depth is a measure of the faintest object (in terms of magnitude) that can be detected in an image with a 5-sigma confidence. 
    metric = rubin_sim.maf.metrics.PassMetric(cols=['filter', 'observationStartMJD', 'fiveSigmaDepth']) # Metrics interested in
    slicer = maf.slicers.UserPointsSlicer(ra=[ra], dec=[dec]) # # Locations interested in - Slice the sky in the point ra,dec
    sql = ''
    bundleMetrics = maf.MetricBundle(metric, slicer, sql) # Metrics ran per slice_point (dict)
    bundle = maf.MetricBundleGroup([bundleMetrics], opsim, out_dir="temp") #This generate ResultsDb with the metrics and stats
    bundle.run_all(plot_now=False) # write metrics in MetricBundle.metric_values #Try plot now to show metrics results
    dataSlice = bundleMetrics.metric_values[0]
    return dataSlice, LSST_BandPass, photParams
# Location of the star
galactic_center = SkyCoord(l=0*u.degree, b=0*u.degree, frame='galactic')
coord = galactic_center.transform_to('icrs')
ra, dec = coord.ra.deg, coord.dec.deg
# Get MJD abd M5
dataSlice, LSST_BandPass, photParams = LSST_cadence_noise(ra, dec)

band ="i"; Merr = {}
MJD, M_input[band] = np.loadtxt('lc_'+band.lower()+'1.dat').T
MJD, M5 = np.loadtxt("lc_i.dat").T 
Merr[band] = [signaltonoise.calc_mag_error_m5(M_input[band],  LSST_BandPass[band.lower()], M5 , photParams)[0] for M5 in M_5] 
plot_lc(MJD, M_input[band] , Merr[band], bands = ["i"], title = f"Cataclysmic Variable - Baseline: {baseline}")

------------------------

# Cut to 3 years and sort the points of the constant light curves
import os
import pandas as pd
import numpy as np
from tqdm import tqdm

folder_input = 'DataSets/trilegal1_i'
folder_output = 'DataSets/Constant'
os.makedirs(folder_output, exist_ok=True)

mjd_limit = 61400

files_txt = [f for f in os.listdir(folder_input) if f.endswith('.txt')]
file_txt = files_txt[0]
for file_txt LSStin tqdm(files_txt):
    input_root = os.path.join(folder_input, file_txt)

    df = pd.read_csv(input_root, sep = " ", header=None, names=['mjd', 'mag', 'mag_err'])

    df_filt = df[df['mjd'] <= mjd_limit]
    df_filt = df_filt.sort_values(by='mjd')

    name_output_file = file_txt.replace('.txt', '.dat')
    output_root = os.path.join(folder_output, name_output_file)

    write_lc_file(df_filt["mjd"], df_filt["mag"], df_filt["mag_err"], output_root)



