{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce5f0d9-c7b4-4d8d-9ba2-330903e9efb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter the lightcurves and save to a new directory which will be input into MicroLIA's training_set module ###\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "original_path = '/Users/daniel/Downloads/ELASTICC2_i_Constant/' # The directory initially provided by Karen containing all the lightcurves for every class\n",
    "new_path = '/Users/daniel/Downloads/ELASTICC2_i_filtered/' # A new directory where the filtered lightcurves will be saved (must contain empty subdirectories corresponding to the below categories!)\n",
    "\n",
    "# All compiled classes, which are the folder names. NOTE: None of the Mdwarf-flare have has_peak and has_variability!\n",
    "categories = ['Cepheid', 'd-Sct', 'uLens-Single_PyLIMA', 'EB', 'RRL', 'uLens-Binary', 'uLens-Single-GenLens', 'Constant'] #'dwarf-nova', 'Mdwarf-flare'\n",
    "\n",
    "num_to_save = 1000 # Will only save 1000 lightcurves per class \n",
    "sigma_depth = 5 # Will compute the 5σ depth to remove measurements 1σ below this limit\n",
    "saturation_limit_i = 15.8 # LSST saturation limit for i-band (https://www.lsst.org/sites/default/files/docs/sciencebook/SB_3.pdf)\n",
    "\n",
    "for category in categories:\n",
    "\tif category != 'Constant':\n",
    "\t\t# Load the lightcurve names and filtering criteria as saved by Karen\n",
    "\t\tfilter_results = np.loadtxt(original_path+'filters_results_'+category+'.txt', skiprows=1, dtype=str)\n",
    "\t\t#\n",
    "\t\t# Set the four recommended criteria\n",
    "\t\tvalid = high_chi2 = has_peak = has_variability = 'True' #columns 2,3,4,5\n",
    "\t\t#\n",
    "\t\t# Index the according to the above criteria \n",
    "\t\tindices = np.where((filter_results[:,2] == valid) & (filter_results[:,3] == high_chi2) & (filter_results[:,4] == has_peak) & (filter_results[:,5] == has_variability))[0]\n",
    "\t\tprint(f'Class: {category} | No. of Good Lightcurves: {len(indices)}')\n",
    "\t\t# These are the names of the good, filtered lightcurves\n",
    "\t\tlc_file_names = np.array(['/'+file.split('/')[-1] for file in filter_results[:,0]])[indices]\n",
    "\telse:\n",
    "\t\t# No need to filter the constant lightcurves, all are assumed to be good\n",
    "\t\tlc_file_names = np.array(['/'+file.split('/')[-1] for file in os.listdir(original_path+category)  if '.dat' in file])\n",
    "\t#\n",
    "\t# Loop through the lightcurve files and save to the new directory\n",
    "\tcounter = 0 # To determine how many good lightcurves are being saved\n",
    "\tfor i in range(len(lc_file_names)):\n",
    "\t\t# Load the lightcurve data\n",
    "\t\tdata = np.loadtxt(original_path+category+lc_file_names[i])\n",
    "\t\tmag, magerr = data[:,1], data[:,2]\n",
    "\t\t#\n",
    "\t\t# Mask out bad data points\n",
    "\t\tfive_sigma_depth = np.mean(mag) + sigma_depth * np.mean(magerr) # The 5σ depth computed with means instead of medians\n",
    "\t\t#\n",
    "\t\t# Select only the measurements brighter than the 5σ depth and dimmer than the saturation limit\n",
    "\t\tmask = np.where(((mag + magerr) < five_sigma_depth) & ((mag - magerr) > saturation_limit_i))[0]\n",
    "\t\t#\n",
    "\t\t# Save the masked lightcurve if it contains at least 10 points\n",
    "\t\tif len(mask) >= 10:\n",
    "\t\t\tnp.savetxt(new_path+category+lc_file_names[i], data[mask])\n",
    "\t\t\tcounter += 1\n",
    "\t\t\tif counter == num_to_save:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\n",
    "### Generate the training set using the training_set.load_all() function, therefore will use the actual saved lightcurves in lieu of simulations ### \n",
    "\n",
    "from MicroLIA import training_set \n",
    "\n",
    "new_path = '/Users/daniel/Downloads/ELASTICC2_i_filtered/' # Where the filtered lightcurves were saved as outlined above\n",
    "\n",
    "zeropoint_i = 27.85 # Instrumental zeropoint for LSST (https://smtn-002.lsst.io)\n",
    "convert = True # To convert from apparent magnitudes to flux\n",
    "apply_weights = True # Will consider the magnitude errors when calculating the lightcurve statistics \n",
    "save_file = True # Whether to save the training set, as by default it only returns it (note that this will always save to the local home directory)\n",
    "filename = 'Filtered_Training_Set_1000' # Pandas dataframe will be saved with the following name: 'MicroLIA_Training_Set_'+filename+'.csv'\n",
    "\n",
    "data_x, data_y = training_set.load_all(new_path, convert=convert, zp=zeropoint_i, filename=filename, apply_weights=apply_weights, save_file=save_file)\n",
    "\n",
    "\n",
    "### Train and optimize the classifiers ###\n",
    "\n",
    "import os\n",
    "import pandas as pd \n",
    "from MicroLIA import ensemble_model\n",
    "\n",
    "# Load the training set file generated by the training_set module\n",
    "filename = 'Filtered_Training_Set_1000'\n",
    "file_path = os.path.expanduser('~/MicroLIA_Training_Set_'+filename+'.csv') # Pandas dataframe that was saved, by design it is always saved in local home!\n",
    "training_set = pd.read_csv(file_path)\n",
    "\n",
    "# Will only use five classes\n",
    "classes_to_use = ['Constant', 'd-Sct', 'EB', 'RRL', 'uLens-Single_PyLIMA']\n",
    "training_set = training_set[training_set['label'].isin(classes_to_use)]\n",
    "\n",
    "# Classifier set up\n",
    "optimize = True # Whether to run the optimization routine\n",
    "n_iter = 250 # Will run the model hyperparameter routine for 250 trials\n",
    "impute = True # Whether to impute NaN values\n",
    "opt_cv = 5 # Will optimize using 5-fold cross-validation\n",
    "boruta_trials = 250 # Will run the feature selection routine for 250 trials\n",
    "\n",
    "# The feature importances will be ranked using an XGBoost model (more conservative than the 'rf' option, both are worth exploring)\n",
    "boruta_model = 'xgb' # Note that this 'xgb' method fails with large datasets, bug is fixed and pushed to GitHub\n",
    "\n",
    "clf = 'xgb' # The model to train, this is the XGBoost\n",
    "\n",
    "model = ensemble_model.Classifier(clf=clf, training_data=training_set, optimize=optimize, \n",
    "\topt_cv=opt_cv, boruta_trials=boruta_trials, boruta_model=boruta_model, n_iter=n_iter, impute=impute)\n",
    "\n",
    "model.create() \n",
    "model.save(dirname=f'Filtered_Training_Set_1000_{clf}_Model')\n",
    "\n",
    "# Now optimize and save a Random Forest model\n",
    "\n",
    "model.clf = 'rf'\n",
    "\n",
    "model.create() \n",
    "model.save(dirname=f'Filtered_Training_Set_1000_{clf}_Model')\n",
    "\n",
    "\n",
    "### Load the saved classification models and review performance ###\n",
    "\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from MicroLIA import ensemble_model\n",
    "\n",
    "# Load the training set file generated by the training_set module\n",
    "filename = 'Filtered_Training_Set_1000'\n",
    "file_path = os.path.expanduser('~/MicroLIA_Training_Set_'+filename+'.csv') # Pandas dataframe that was saved, by design it is always saved in local home!\n",
    "training_set = pd.read_csv(file_path)\n",
    "\n",
    "# Will only use five classes\n",
    "classes_to_use = ['Constant', 'd-Sct', 'EB', 'RRL', 'uLens-Single_PyLIMA']\n",
    "training_set = training_set[training_set['label'].isin(classes_to_use)]\n",
    "\n",
    "# Classifier set up -- will investigate the saved XGBoost model for the below example\n",
    "clf = 'xgb'\n",
    "opt_cv = 5 # Will optimize using 5-fold cross-validation\n",
    "impute = True # Whether to impute NaN values\n",
    "\n",
    "# Instantiate the Classifier and use the load method, note that the load path corresponds to the dirname I used above when saving the model\n",
    "model = ensemble_model.Classifier(clf=clf, training_data=training_set, opt_cv=opt_cv, impute=impute)\n",
    "model.load(path=f'Filtered_Training_Set_1000_{clf}_Model')\n",
    "\n",
    "# Can output the following plots\n",
    "model.plot_hyper_opt(ylim=(0.84,0.9))\n",
    "model.plot_conf_matrix(k_fold=opt_cv, title='XGBoost Model (5-Fold CV)', savefig=False)\n",
    "model.plot_roc_curve(k_fold=opt_cv)\n",
    "model.plot_feature_opt(feat_names='default', flip_axes=True, savefig=False)\n",
    "model.plot_tsne()\n",
    "\n",
    "## ## \n",
    "## ##\n",
    "## ## \n",
    "\n",
    "### Below I show how to make new predictions, we will predict the simulated best-score lightcurves ###\n",
    "\n",
    "# Let's classify all the single lens events in the best scored dataset shared by Karen \n",
    "all_single_lenses = '/Users/daniel/Downloads/ELAsTiCC_datasets/ELASTICC2_i_Valid_Chi_Constant/uLens-Single_PyLIMA/'\n",
    "fnames = [fname for fname in os.listdir(all_single_lenses) if '.dat' in fname]\n",
    "\n",
    "zeropoint_i = 27.85 # Instrumental zeropoint for LSST (https://smtn-002.lsst.io)\n",
    "convert = True # To convert from apparent magnitudes to flux\n",
    "apply_weights = True # Will consider the magnitude errors when calculating the lightcurve statistics \n",
    "\n",
    "# We can store all the predictions in a single 3-D array\n",
    "predictions_single_lens = np.zeros((len(fnames), len(classes_to_use), 2)) # The last axis is 2 because the prediction method returns two columns, the class label and corresponding probability prediction\n",
    "\n",
    "for i in range(len(fnames)):\n",
    "\t#print(f\"{i+1} out of {len(fnames)}\")\n",
    "\tlightcurve = np.loadtxt(all_single_lenses+fnames[i])\n",
    "\ttime, mag, magerr = lightcurve[:,0], lightcurve[:,1], lightcurve[:,2]\n",
    "\t#\n",
    "\t# The model's built-in prediction method, outputs 2-D array (class label and corresponding probability prediction)\n",
    "\tpredictions_single_lens[i] = model.predict(time, mag, magerr, convert=convert, apply_weights=apply_weights, zp=zeropoint_i)\n",
    "\n",
    "# The classes used to generate the above model were: ['Constant', 'd-Sct', 'EB', 'RRL', 'uLens-Single_PyLIMA']\n",
    "# These are converted to numerical labels for the XGBoost model by design\n",
    "# Furthermore these are sorted in alphabetical order therefore: Constant=0, EB=1, RRL=2, d-Sct=3, uLens-Single_PyLIMA=4\n",
    "\n",
    "# Check how many predictions were output as uLens-Single_PyLIMA by checking whether the highest probability prediction corresponds to the label 4\n",
    "\n",
    "positive_detections_single = 0\n",
    "for i in range(len(predictions_single_lens)):\n",
    "\t# Checks whether the highest probability is in the row corresponding to uLens-Single_PyLIMA (row 4)\n",
    "\tif np.argmax(predictions_single_lens[i][:,1]) == 4: #The second column holds the probability predictions\n",
    "\t\tpositive_detections_single += 1\n",
    "\n",
    "print(f\"Correctly classified {positive_detections_single} out of {len(predictions_single_lens)} uLens-Single_PyLIMA ({np.round(100*(positive_detections_single/len(predictions_single_lens)),4)}%)\")\n",
    "\n",
    "## ## \n",
    "\n",
    "# Now check how many of the binary lenses (which were not used for training) are classified as single lenses\n",
    "all_binary_lenses = '/Users/daniel/Downloads/ELAsTiCC_datasets/ELASTICC2_i_Valid_Chi_Constant/uLens-Binary/'\n",
    "fnames = [fname for fname in os.listdir(all_binary_lenses) if '.dat' in fname]\n",
    "\n",
    "# We can store all the predictions in a single 3-D array\n",
    "predictions_binary_lens = np.zeros((len(fnames), len(classes_to_use), 2)) # The last axis is 2 because the prediction method returns two columns, the class label and corresponding probability prediction\n",
    "\n",
    "for i in range(len(fnames)):\n",
    "\t#print(f\"{i+1} out of {len(fnames)}\")\n",
    "\tlightcurve = np.loadtxt(all_binary_lenses+fnames[i])\n",
    "\ttime, mag, magerr = lightcurve[:,0], lightcurve[:,1], lightcurve[:,2]\n",
    "\t#\n",
    "\t# The model's built-in prediction method, outputs 2-D array (class label and corresponding probability prediction)\n",
    "\tpredictions_binary_lens[i] = model.predict(time, mag, magerr, convert=convert, apply_weights=apply_weights, zp=zeropoint_i)\n",
    "\n",
    "\n",
    "# Check how many predictions were output as uLens-Single_PyLIMA by checking whether the highest probability prediction corresponds to the label 4\n",
    "positive_detections_binary = 0\n",
    "for i in range(len(predictions_binary_lens)):\n",
    "\t# Checks whether the highest probability is in the row corresponding to uLens-Single_PyLIMA (row 4)\n",
    "\tif np.argmax(predictions_binary_lens[i][:,1]) == 4: #The second column holds the probability predictions\n",
    "\t\tpositive_detections_binary += 1\n",
    "\n",
    "print(f\"Correctly classified {positive_detections_binary} out of {len(predictions_binary_lens)} uLens-Binary ({np.round(100*(positive_detections_binary/len(predictions_binary_lens)),4)}%)\")\n",
    "\n",
    "## ## \n",
    "\n",
    "# Now check how many of the single gen lenses (which were not used for training) are classified as single lenses\n",
    "\n",
    "all_single_gen_lens = '/Users/daniel/Downloads/ELAsTiCC_datasets/ELASTICC2_i_Valid_Chi_Constant/uLens-Single-GenLens/'\n",
    "fnames = [fname for fname in os.listdir(all_single_gen_lens) if '.dat' in fname]\n",
    "\n",
    "# We can store all the predictions in a single 3-D array\n",
    "predictions_single_gen_lens = np.zeros((len(fnames), len(classes_to_use), 2)) # The last axis is 2 because the prediction method returns two columns, the class label and corresponding probability prediction\n",
    "\n",
    "for i in range(len(fnames)):\n",
    "\t#print(f\"{i+1} out of {len(fnames)}\")\n",
    "\tlightcurve = np.loadtxt(all_single_gen_lens+fnames[i])\n",
    "\ttime, mag, magerr = lightcurve[:,0], lightcurve[:,1], lightcurve[:,2]\n",
    "\t#\n",
    "\t# The model's built-in prediction method, outputs 2-D array (class label and corresponding probability prediction)\n",
    "\tpredictions_single_gen_lens[i] = model.predict(time, mag, magerr, convert=convert, apply_weights=apply_weights, zp=zeropoint_i)\n",
    "\n",
    "\n",
    "# Check how many predictions were output as uLens-Single_PyLIMA by checking whether the highest probability prediction corresponds to the label 4\n",
    "positive_detections_gen_lens = 0\n",
    "for i in range(len(predictions_single_gen_lens)):\n",
    "\t# Checks whether the highest probability is in the row corresponding to uLens-Single_PyLIMA (row 4)\n",
    "\tif np.argmax(predictions_single_gen_lens[i][:,1]) == 4: #The second column holds the probability predictions\n",
    "\t\tpositive_detections_gen_lens += 1\n",
    "\n",
    "print(f\"Correctly classified {positive_detections_gen_lens} out of {len(predictions_single_gen_lens)} uLens-Single-GenLens ({np.round(100*(positive_detections_gen_lens/len(predictions_single_gen_lens)),4)}%)\")\n",
    "\n",
    "## ## \n",
    "\n",
    "# Plot a histogram to visualize the probability predictions of all three microlensing classes\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the microlensing probabilities only for all predictions (column 1, row 4)\n",
    "single_lens_probas = predictions_single_lens[:, 4, 1]\n",
    "binary_lens_probas = predictions_binary_lens[:, 4, 1]\n",
    "single_gen_lens_probas = predictions_single_gen_lens[:, 4, 1]\n",
    "\n",
    "# Histograms\n",
    "hist_single_lens, bins_single_lens = np.histogram(single_lens_probas, bins=120)\n",
    "hist_single_gen_lens, bins_single_gen_lens = np.histogram(single_gen_lens_probas, bins=120)\n",
    "hist_binary_lens, bins_binary_lens = np.histogram(binary_lens_probas, bins=120)\n",
    "\n",
    "# Normalize the histograms by dividing by the total number of samples\n",
    "hist_single_lens_normalized = hist_single_lens / len(single_lens_probas)\n",
    "hist_single_gen_lens_normalized = hist_single_gen_lens / len(single_gen_lens_probas)\n",
    "hist_binary_lens_normalized = hist_binary_lens / len(binary_lens_probas)\n",
    "\n",
    "# Plot the normalized histograms\n",
    "plt.bar(bins_single_lens[:-1], hist_single_lens_normalized, width=(bins_single_lens[1] - bins_single_lens[0]), alpha=0.5, label=f'uLens-Single_PyLIMA (n={len(predictions_single_lens)}, Tot. Acc: {np.round(100*(positive_detections_single/len(predictions_single_lens)),2)}%)')\n",
    "plt.bar(bins_single_gen_lens[:-1], hist_single_gen_lens_normalized, width=(bins_single_gen_lens[1] - bins_single_gen_lens[0]), alpha=0.7, label=f'uLens-Single-GenLens (n={len(predictions_single_gen_lens)}, Tot. Acc: {np.round(100*(positive_detections_gen_lens /len(predictions_single_gen_lens)),2)}%)')\n",
    "plt.bar(bins_binary_lens[:-1], hist_binary_lens_normalized, width=(bins_binary_lens[1] - bins_binary_lens[0]), alpha=0.55, label=f'uLens-Binary (n={len(predictions_binary_lens)}, Tot. Acc: {np.round(100*(positive_detections_binary /len(predictions_binary_lens)),2)}%)')\n",
    "plt.title('Classification of Microlensing Classes (Optimized XGBoost Model)')\n",
    "plt.xlabel('Probability Prediction Lightcurve is a uLens-Single_PyLIMA'); plt.ylabel('Normalized Counts')\n",
    "plt.ylim(0, 1); plt.xlim(0.8, 1); plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "### The exact same analysis as above but now examining the RF model ###\n",
    "# Main difference is that unlike XGBoost, the RF model can have non-numerical class labels #\n",
    "# If the class labels are strings, then the prediction method will return strings instead of floats including the probability prediction #\n",
    "\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from MicroLIA import ensemble_model\n",
    "\n",
    "# Load the training set file generated by the training_set module\n",
    "filename = 'Filtered_Training_Set_1000'\n",
    "file_path = os.path.expanduser('~/MicroLIA_Training_Set_'+filename+'.csv') # Pandas dataframe that was saved, by design it is always saved in local home!\n",
    "training_set = pd.read_csv(file_path)\n",
    "\n",
    "# Will only use five classes\n",
    "classes_to_use = ['Constant', 'd-Sct', 'EB', 'RRL', 'uLens-Single_PyLIMA']\n",
    "training_set = training_set[training_set['label'].isin(classes_to_use)]\n",
    "\n",
    "# Classifier set up -- will investigate the saved XGBoost model for the below example\n",
    "clf = 'rf'\n",
    "opt_cv = 5 # Will optimize using 5-fold cross-validation\n",
    "impute = True # Whether to impute NaN values\n",
    "\n",
    "# Instantiate the Classifier and use the load method, note that the load path corresponds to the dirname I used above when saving the model\n",
    "model = ensemble_model.Classifier(clf=clf, training_data=training_set, opt_cv=opt_cv, impute=impute)\n",
    "model.load(path=f'Filtered_Training_Set_1000_{clf}_Model')\n",
    "\n",
    "# Can output the following plots\n",
    "model.plot_hyper_opt(ylim=(0.8,0.9))\n",
    "model.plot_conf_matrix(k_fold=opt_cv, title='RF Model (5-Fold CV)', savefig=False)\n",
    "model.plot_roc_curve(k_fold=opt_cv)\n",
    "model.plot_feature_opt(feat_names='default', flip_axes=True, savefig=False) #Same as the first XGBoost model since the feature selection was the same\n",
    "model.plot_tsne() #Same as the first XGBoost model since the feature selection was the same\n",
    "\n",
    "## ## \n",
    "## ##\n",
    "## ## \n",
    "\n",
    "### Below I show how to make new predictions, we will predict the simulated best-score lightcurves ###\n",
    "\n",
    "# Let's classify all the single lens events in the best scored dataset shared by Karen \n",
    "all_single_lenses = '/Users/daniel/Downloads/ELAsTiCC_datasets/ELASTICC2_i_Valid_Chi_Constant/uLens-Single_PyLIMA/'\n",
    "fnames = [fname for fname in os.listdir(all_single_lenses) if '.dat' in fname]\n",
    "\n",
    "zeropoint_i = 27.85 # Instrumental zeropoint for LSST (https://smtn-002.lsst.io)\n",
    "convert = True # To convert from apparent magnitudes to flux\n",
    "apply_weights = True # Will consider the magnitude errors when calculating the lightcurve statistics \n",
    "\n",
    "# We can store all the predictions in a single 3-D array but in this case will use np.empty instead of np.zeros since we will be storing strings since the RF model was trained with string class labels\n",
    "predictions_single_lens = np.empty((len(fnames), len(classes_to_use), 2), dtype=np.dtype('U50'))  # 'U50' specifies a Unicode string with a maximum length of 50 characters\n",
    "\n",
    "for i in range(len(fnames)):\n",
    "\t#print(f\"{i+1} out of {len(fnames)}\")\n",
    "\tlightcurve = np.loadtxt(all_single_lenses+fnames[i])\n",
    "\ttime, mag, magerr = lightcurve[:,0], lightcurve[:,1], lightcurve[:,2]\n",
    "\t#\n",
    "\t# The model's built-in prediction method, outputs 2-D array (class label and corresponding probability prediction)\n",
    "\tpredictions_single_lens[i] = model.predict(time, mag, magerr, convert=convert, apply_weights=apply_weights, zp=zeropoint_i)\n",
    "\n",
    "\n",
    "# Check how many predictions were output as uLens-Single_PyLIMA by checking whether the highest probability prediction corresponds to the label 4\n",
    "\n",
    "positive_detections_single = 0\n",
    "for i in range(len(predictions_single_lens)):\n",
    "\t# Checks whether the highest probability is in the row corresponding to uLens-Single_PyLIMA (row 4)\n",
    "\tindex_highest_proba = np.argmax(predictions_single_lens[i][:,1].astype('float')) #The second column holds the probability predictions\n",
    "\tif predictions_single_lens[i][:,0][index_highest_proba] == 'uLens-Single_PyLIMA': #The first column are the labels\n",
    "\t\tpositive_detections_single += 1\n",
    "\n",
    "print(f\"Correctly classified {positive_detections_single} out of {len(predictions_single_lens)} uLens-Single_PyLIMA ({np.round(100*(positive_detections_single/len(predictions_single_lens)),4)}%)\")\n",
    "\n",
    "## ##\n",
    "\n",
    "# Now check how many of the binary lenses (which were not used for training) are classified as single lenses\n",
    "all_binary_lenses = '/Users/daniel/Downloads/ELAsTiCC_datasets/ELASTICC2_i_Valid_Chi_Constant/uLens-Binary/'\n",
    "fnames = [fname for fname in os.listdir(all_binary_lenses) if '.dat' in fname]\n",
    "\n",
    "# We can store all the predictions in a single 3-D array\n",
    "predictions_binary_lens = np.empty((len(fnames), len(classes_to_use), 2), dtype=np.dtype('U50'))  # 'U50' specifies a Unicode string with a maximum length of 50 characters\n",
    "\n",
    "for i in range(len(fnames)):\n",
    "\t#print(f\"{i+1} out of {len(fnames)}\")\n",
    "\tlightcurve = np.loadtxt(all_binary_lenses+fnames[i])\n",
    "\ttime, mag, magerr = lightcurve[:,0], lightcurve[:,1], lightcurve[:,2]\n",
    "\t#\n",
    "\t# The model's built-in prediction method, outputs 2-D array (class label and corresponding probability prediction)\n",
    "\tpredictions_binary_lens[i] = model.predict(time, mag, magerr, convert=convert, apply_weights=apply_weights, zp=zeropoint_i)\n",
    "\n",
    "\n",
    "# Check how many predictions were output as uLens-Single_PyLIMA by checking whether the highest probability prediction corresponds to the label 4\n",
    "positive_detections_binary = 0\n",
    "for i in range(len(predictions_binary_lens)):\n",
    "\t# Checks whether the highest probability is in the row corresponding to uLens-Single_PyLIMA (row 4)\n",
    "\tindex_highest_proba = np.argmax(predictions_binary_lens[i][:,1].astype('float')) #The second column holds the probability predictions\n",
    "\tif predictions_binary_lens[i][:,0][index_highest_proba] == 'uLens-Single_PyLIMA': #The first column are the labels\n",
    "\t\tpositive_detections_binary += 1\n",
    "\n",
    "print(f\"Correctly classified {positive_detections_binary} out of {len(predictions_binary_lens)} uLens-Binary ({np.round(100*(positive_detections_binary/len(predictions_binary_lens)),4)}%)\")\n",
    "\n",
    "## ##\n",
    "\n",
    "# Now check how many of the single gen lenses (which were not used for training) are classified as single lenses\n",
    "\n",
    "all_single_gen_lens = '/Users/daniel/Downloads/ELAsTiCC_datasets/ELASTICC2_i_Valid_Chi_Constant/uLens-Single-GenLens/'\n",
    "fnames = [fname for fname in os.listdir(all_single_gen_lens) if '.dat' in fname]\n",
    "\n",
    "# We can store all the predictions in a single 3-D array\n",
    "predictions_single_gen_lens = np.empty((len(fnames), len(classes_to_use), 2), dtype=np.dtype('U50')) # The last axis is 2 because the prediction method returns two columns, the class label and corresponding probability prediction\n",
    "\n",
    "for i in range(len(fnames)):\n",
    "\t#print(f\"{i+1} out of {len(fnames)}\")\n",
    "\tlightcurve = np.loadtxt(all_single_gen_lens+fnames[i])\n",
    "\ttime, mag, magerr = lightcurve[:,0], lightcurve[:,1], lightcurve[:,2]\n",
    "\t#\n",
    "\t# The model's built-in prediction method, outputs 2-D array (class label and corresponding probability prediction)\n",
    "\tpredictions_single_gen_lens[i] = model.predict(time, mag, magerr, convert=convert, apply_weights=apply_weights, zp=zeropoint_i)\n",
    "\n",
    "\n",
    "# Check how many predictions were output as uLens-Single_PyLIMA by checking whether the highest probability prediction corresponds to the label 4\n",
    "positive_detections_gen_lens = 0\n",
    "for i in range(len(predictions_single_gen_lens)):\n",
    "\t# Checks whether the highest probability is in the row corresponding to uLens-Single_PyLIMA (row 4)\n",
    "\tindex_highest_proba = np.argmax(predictions_single_gen_lens[i][:,1].astype('float')) #The second column holds the probability predictions\n",
    "\tif predictions_single_gen_lens[i][:,0][index_highest_proba] == 'uLens-Single_PyLIMA': #The first column are the labels\n",
    "\t\tpositive_detections_gen_lens += 1\n",
    "\n",
    "print(f\"Correctly classified {positive_detections_gen_lens} out of {len(predictions_single_gen_lens)} uLens-Single-GenLens ({np.round(100*(positive_detections_gen_lens/len(predictions_single_gen_lens)),4)}%)\")\n",
    "\n",
    "## ##\n",
    "\n",
    "# Plot a histogram to visualize the probability predictions of all three microlensing classes\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the microlensing probabilities only for all predictions (column 1, row 4)\n",
    "single_lens_probas = predictions_single_lens[:, 4, 1].astype('float')\n",
    "binary_lens_probas = predictions_binary_lens[:, 4, 1].astype('float')\n",
    "single_gen_lens_probas = predictions_single_gen_lens[:, 4, 1].astype('float')\n",
    "\n",
    "# Histograms\n",
    "hist_single_lens, bins_single_lens = np.histogram(single_lens_probas, bins=120)\n",
    "hist_single_gen_lens, bins_single_gen_lens = np.histogram(single_gen_lens_probas, bins=120)\n",
    "hist_binary_lens, bins_binary_lens = np.histogram(binary_lens_probas, bins=120)\n",
    "\n",
    "# Normalize the histograms by dividing by the total number of samples\n",
    "hist_single_lens_normalized = hist_single_lens / len(single_lens_probas)\n",
    "hist_single_gen_lens_normalized = hist_single_gen_lens / len(single_gen_lens_probas)\n",
    "hist_binary_lens_normalized = hist_binary_lens / len(binary_lens_probas)\n",
    "\n",
    "# Plot the normalized histograms\n",
    "plt.bar(bins_single_lens[:-1], hist_single_lens_normalized, width=(bins_single_lens[1] - bins_single_lens[0]), alpha=0.5, label=f'uLens-Single_PyLIMA (n={len(predictions_single_lens)}, Tot. Acc: {np.round(100*(positive_detections_single/len(predictions_single_lens)),2)}%)')\n",
    "plt.bar(bins_single_gen_lens[:-1], hist_single_gen_lens_normalized, width=(bins_single_gen_lens[1] - bins_single_gen_lens[0]), alpha=0.7, label=f'uLens-Single-GenLens (n={len(predictions_single_gen_lens)}, Tot. Acc: {np.round(100*(positive_detections_gen_lens /len(predictions_single_gen_lens)),2)}%)')\n",
    "plt.bar(bins_binary_lens[:-1], hist_binary_lens_normalized, width=(bins_binary_lens[1] - bins_binary_lens[0]), alpha=0.55, label=f'uLens-Binary (n={len(predictions_binary_lens)}, Tot. Acc: {np.round(100*(positive_detections_binary /len(predictions_binary_lens)),2)}%)')\n",
    "plt.title('Classification of Microlensing Classes (Optimized RF Model)')\n",
    "plt.xlabel('Probability Prediction Lightcurve is a uLens-Single_PyLIMA'); plt.ylabel('Normalized Counts')\n",
    "plt.ylim(0, 1); plt.xlim(0.8, 1); plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
